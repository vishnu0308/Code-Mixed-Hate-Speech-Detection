{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd ~/vishnu/aws_files\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install tensorflow\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import pandas as pd\r\n",
    "import regex\r\n",
    "from library import *\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trac1_dataset_preprocess():\r\n",
    "    df1 = load_dataset(\"agr_hi_train.csv\")\r\n",
    "    df2 = load_dataset(\"agr_en_train.csv\")\r\n",
    "    df = pd.concat([df1, df2])\r\n",
    "    df = preprocess_text(df)\r\n",
    "    df[\"message\"].fillna('', inplace=True)\r\n",
    "    df1 = load_dataset(\"agr_hi_dev.csv\")\r\n",
    "    df2 = load_dataset(\"agr_en_dev.csv\")\r\n",
    "    val_df = pd.concat([df1, df2])\r\n",
    "    val_df = preprocess_text(val_df)\r\n",
    "    y_train = df[\"class\"]\r\n",
    "    y_test = val_df[\"class\"]\r\n",
    "    x_train = df[\"message\"]\r\n",
    "    x_test = val_df[\"message\"]\r\n",
    "    return x_train, x_test, y_train, y_test\r\n",
    "\r\n",
    "\r\n",
    "x_train, x_test, y_train, y_test = trac1_dataset_preprocess()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Concatenate, Dropout, GlobalMaxPooling1D, concatenate, Reshape\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_max_text_len(msgs):\r\n",
    "    return max(list(map(lambda msg: len(msg), msgs)))\r\n",
    "\r\n",
    "\r\n",
    "def convert_classes_to_nums(y_train, y_test):\r\n",
    "    classes = y_train\r\n",
    "    le = LabelEncoder()\r\n",
    "    integer_labels = le.fit_transform(classes)\r\n",
    "    y_train = integer_labels\r\n",
    "    y_test = le.transform(y_test)\r\n",
    "    return y_train, y_test, le\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define the path to the pre-trained GloVe model file\r\n",
    "glove_file = 'glove.twitter.27B.200d.txt'\r\n",
    "\r\n",
    "# Define the format of the pre-trained GloVe model file\r\n",
    "\r\n",
    "\r\n",
    "def read_glove_file(glove_file):\r\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    word_to_vec = {}\r\n",
    "    for line in lines:\r\n",
    "        line = line.strip().split()\r\n",
    "        word = line[0]\r\n",
    "        vec = np.array([float(val) for val in line[1:]])\r\n",
    "        word_to_vec[word] = vec\r\n",
    "    return word_to_vec\r\n",
    "\r\n",
    "\r\n",
    "# Load the pre-trained GloVe model\r\n",
    "word_to_vec_map = read_glove_file(glove_file)\r\n",
    "\r\n",
    "glove_embed_dict = word_to_vec_map\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
    "\r\n",
    "\r\n",
    "def print_classification_metrics(y_pred, y_test):\r\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\r\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\r\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\r\n",
    "    print(\"Precision: {:.2f}\".format(precision))\r\n",
    "    print(\"Recall: {:.2f}\".format(recall))\r\n",
    "    print(\"F1-score: {:.2f}\".format(f1))\r\n",
    "\r\n",
    "\r\n",
    "def infer_class(model, tokenizer, max_len, label_encoder, text):\r\n",
    "    # Tokenize the input text\r\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\r\n",
    "    # Pad the sequence to the maximum length\r\n",
    "    padded_sequence = pad_sequences(text_sequence, maxlen=max_len)\r\n",
    "    # Make the prediction\r\n",
    "    prediction = model.predict(padded_sequence, verbose=0)[0]\r\n",
    "    # Convert the prediction to the actual label\r\n",
    "    predicted_label = np.argmax(prediction)\r\n",
    "#     predicted_class = label_encoder.inverse_transform([predicted_label])[0]\r\n",
    "    return predicted_label\r\n",
    "\r\n",
    "\r\n",
    "def predict_all(model, tokenizer, max_len, label_encoder, x_test):\r\n",
    "    y_pred = []\r\n",
    "    for msg in list(x_test):\r\n",
    "        pred_class = infer_class(model, tokenizer, max_len, label_encoder, msg)\r\n",
    "        y_pred.append(pred_class)\r\n",
    "    return y_pred\r\n",
    "\r\n",
    "\r\n",
    "def additional_metrics(model, tokenizer, max_len, le):\r\n",
    "    x_train, x_test, y_train, y_test = trac1_dataset_preprocess()\r\n",
    "    y_test = le.transform(y_test)\r\n",
    "    y_pred = predict_all(model, tokenizer, max_len, le, x_test)\r\n",
    "    print_classification_metrics(y_pred, y_test)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling1D, Dense, multiply, GlobalMaxPooling1D, Lambda\r\n",
    "from keras import backend as K\r\n",
    "\r\n",
    "\r\n",
    "def se_block(in_block, ch, ratio=16):\r\n",
    "    x = GlobalAveragePooling1D()(in_block)\r\n",
    "    x = Dense(ch//ratio, activation='relu')(x)\r\n",
    "    x = Dense(ch, activation='sigmoid')(x)\r\n",
    "    x = Reshape((1, num_filters))(x)\r\n",
    "    tile_layer = Lambda(lambda x: K.tile(x, [1, in_block.shape[1], 1]))(x)\r\n",
    "    x = multiply([in_block, tile_layer])\r\n",
    "    return x\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import save_model, load_model\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
    "x_train, x_test, y_train, y_test = trac1_dataset_preprocess()\r\n",
    "\r\n",
    "embed_dict = glove_embed_dict\r\n",
    "\r\n",
    "text_data = x_train\r\n",
    "val_text_data = x_test\r\n",
    "# Tokenize the text data\r\n",
    "tokenizer = Tokenizer()\r\n",
    "tokenizer.fit_on_texts(text_data)\r\n",
    "word_index = tokenizer.word_index\r\n",
    "\r\n",
    "# Convert text to sequences\r\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\r\n",
    "val_sequences = tokenizer.texts_to_sequences(val_text_data)\r\n",
    "# Pad sequences to a fixed length\r\n",
    "max_len = get_max_text_len(x_train)  # Set the maximum sequence length\r\n",
    "data = pad_sequences(sequences, maxlen=max_len)\r\n",
    "val_data = pad_sequences(val_sequences, maxlen=max_len)\r\n",
    "\r\n",
    "# Convert labels to one-hot encoding\r\n",
    "y_train, y_test, le = convert_classes_to_nums(y_train, y_test)\r\n",
    "labels = to_categorical(y_train)\r\n",
    "test_labels = to_categorical(y_test)\r\n",
    "\r\n",
    "#embedding matrix\r\n",
    "embedding_dim = len(list(embed_dict.values())[0])\r\n",
    "num_words = len(tokenizer.word_index) + 1\r\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\r\n",
    "for word, i in tokenizer.word_index.items():\r\n",
    "    if word.lower() in embed_dict:\r\n",
    "        embedding_matrix[i] = np.array(embed_dict[word.lower()])\r\n",
    "\r\n",
    "embedding_layer = Embedding(num_words, embedding_dim, weights=[\r\n",
    "                            embedding_matrix], trainable=True)\r\n",
    "\r\n",
    "x_train, y_train = data, labels\r\n",
    "x_test, y_test = val_data, test_labels\r\n",
    "\r\n",
    "\r\n",
    "# Define hyperparameters\r\n",
    "embed_dim = embedding_dim\r\n",
    "# embed_dim=100\r\n",
    "num_filters = 64\r\n",
    "filter_sizes = [2, 3, 4]\r\n",
    "dropout_rate = 0.5\r\n",
    "batch_size = 64\r\n",
    "epochs = 5\r\n",
    "\r\n",
    "print(max_len)\r\n",
    "# Define input layer\r\n",
    "input_layer = Input(shape=(max_len,))\r\n",
    "\r\n",
    "# Add embedding layer\r\n",
    "embedding = embedding_layer(input_layer)\r\n",
    "print(embedding.shape)\r\n",
    "\r\n",
    "# Add parallel convolutional layers with max pooling and global max pooling\r\n",
    "conv_layers = []\r\n",
    "for filter_size in filter_sizes:\r\n",
    "    conv_layer = Conv1D(filters=num_filters,\r\n",
    "                        kernel_size=filter_size, activation='relu')(embedding)\r\n",
    "    print(conv_layer.shape)\r\n",
    "    se_layer = se_block(conv_layer, num_filters)\r\n",
    "    pool_layer = MaxPooling1D(pool_size=max_len - filter_size + 1)(se_layer)\r\n",
    "    conv_layers.append(GlobalMaxPooling1D()(pool_layer))\r\n",
    "concat_layer = concatenate(conv_layers, axis=1)\r\n",
    "\r\n",
    "# Add dropout layer\r\n",
    "dropout_layer = Dropout(dropout_rate)(concat_layer)\r\n",
    "\r\n",
    "# Add output layer\r\n",
    "output_layer = Dense(3, activation='softmax')(dropout_layer)\r\n",
    "\r\n",
    "# Define model\r\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
    "\r\n",
    "# Compile model with binary cross-entropy loss and Adam optimizer\r\n",
    "model.compile(loss='categorical_crossentropy',\r\n",
    "              optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "print(model.summary())\r\n",
    "\r\n",
    "\r\n",
    "# define the ModelCheckpoint callback\r\n",
    "checkpoint = ModelCheckpoint('concat_glove_200_cnn1d_seblock_orig.h5',\r\n",
    "                             monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=True)\r\n",
    "\r\n",
    "# train the model\r\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=8,\r\n",
    "          validation_data=(x_test, y_test), callbacks=[checkpoint])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define input layer\r\n",
    "input_layer = Input(shape=(max_len,))\r\n",
    "\r\n",
    "# Add embedding layer\r\n",
    "embedding = embedding_layer(input_layer)\r\n",
    "print(embedding.shape)\r\n",
    "\r\n",
    "# Add parallel convolutional layers with max pooling and global max pooling\r\n",
    "conv_layers = []\r\n",
    "for filter_size in filter_sizes:\r\n",
    "    conv_layer = Conv1D(filters=num_filters,\r\n",
    "                        kernel_size=filter_size, activation='relu')(embedding)\r\n",
    "    print(conv_layer.shape)\r\n",
    "    se_layer = se_block(conv_layer, num_filters)\r\n",
    "    pool_layer = MaxPooling1D(pool_size=max_len - filter_size + 1)(se_layer)\r\n",
    "    conv_layers.append(GlobalMaxPooling1D()(pool_layer))\r\n",
    "concat_layer = concatenate(conv_layers, axis=1)\r\n",
    "\r\n",
    "# Add dropout layer\r\n",
    "dropout_layer = Dropout(dropout_rate)(concat_layer)\r\n",
    "\r\n",
    "# Add output layer\r\n",
    "output_layer = Dense(3, activation='softmax')(dropout_layer)\r\n",
    "\r\n",
    "# Define model\r\n",
    "best_model = Model(inputs=input_layer, outputs=output_layer)\r\n",
    "\r\n",
    "# Compile model with binary cross-entropy loss and Adam optimizer\r\n",
    "best_model.compile(loss='categorical_crossentropy',\r\n",
    "                   optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "best_model.load_weights('concat_glove_200_cnn1d_seblock_orig.h5')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "additional_metrics(best_model, tokenizer, max_len, le)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}